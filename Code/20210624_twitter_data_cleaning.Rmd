---
title: "Twitter Data Cleaning"
author: "Jakob Woerner"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: 'united'
    highlight: 'tango'
    toc: yes
    toc_float: true
    code_folding: 'show'
    df_print: 'paged'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment = NA)
```

# Setup
```{r, warning=FALSE, message=FALSE}
library(magrittr);library(tidytext);library(tidyverse)
```

# Goals

We want to clean the raw twitter data to be something useful that we can use in an algorithm.

# Methods

We will follow the steps from Juejue's thesis, including converting to lowercase, removing symbols and stop words, stemming, and removing URLs.

# Load Data

```{r, warning=TRUE, message=FALSE}
vaccine_passport_500 <- read_csv("data/vaccine_passport_500.csv")
```

# Clean Data

Remove first 'b'

```{r}
vp1 <- vaccine_passport_500 %>% 
  mutate(clean_text = str_sub(text, 2)) %>% 
  select(clean_text, everything())
```

Remove URLs

```{r}
vp2 <- vp1 %>% 
  mutate(clean_text = gsub("http.*","",  clean_text)) %>% 
  mutate(clean_text = gsub("https.*","",  clean_text))
```

Lowercase

```{r}
vp3 <- vp2 %>% 
  mutate(clean_text = tolower(clean_text))
```

Remove Special characters

```{r}
vp4 <- vp3
vp4$clean_text <- gsub("[^0-9A-Za-z///' ]","'" , vp4$clean_text ,ignore.case = TRUE)
vp4$clean_text <- gsub("''","" , vp4$clean_text, ignore.case = TRUE)
vp4$clean_text <- gsub("'", '', vp4$clean_text)
```


Stemming

So far - I just turned it into a list of vectors of words

```{r}
list_of_tweets <- vp4$clean_text %>% as.list()
list_of_vectors <- vector(mode = "list", length = length(list_of_tweets))
for (i in 1:length(list_of_tweets)) {
  list_of_vectors[[i]] <- as.vector(str_split(list_of_tweets[[i]][1], " ")[[1]])
}
```



# Session Information
```{r}
sessioninfo::session_info()
```


